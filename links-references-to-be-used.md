---
description: These will be used to flesh out some other pages
---

# To be added elsewhere

* [https://madhuramiah.medium.com/how-i-increased-the-accuracy-of-mnist-prediction-from-84-to-99-41-63ebd90cc8a0](https://madhuramiah.medium.com/how-i-increased-the-accuracy-of-mnist-prediction-from-84-to-99-41-63ebd90cc8a0)
* [https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network](https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network)
* [https://www.tensorflow.org/api\_docs/python/tf/keras/activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations)
* [https://stackoverflow.com/questions/41175401/what-is-a-batch-in-tensorflow](https://stackoverflow.com/questions/41175401/what-is-a-batch-in-tensorflow)
* [https://medium.com/@mjbhobe/classifying-fashion-with-a-keras-cnn-achieving-94-accuracy-part-2-a5bd7a4e7e5a](https://medium.com/@mjbhobe/classifying-fashion-with-a-keras-cnn-achieving-94-accuracy-part-2-a5bd7a4e7e5a)
* [https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)
* [https://www.kaggle.com/toldo171/tutorial-how-to-get-99-2-from-scratch-indepth](https://www.kaggle.com/toldo171/tutorial-how-to-get-99-2-from-scratch-indepth)
* [https://stats.stackexchange.com/questions/419751/why-is-softmax-function-used-to-calculate-probabilities-although-we-can-divide-e](https://stats.stackexchange.com/questions/419751/why-is-softmax-function-used-to-calculate-probabilities-although-we-can-divide-e)
* Future project idea: celluar automata
* [https://stackoverflow.com/questions/44164749/how-does-keras-handle-multilabel-classification](https://stackoverflow.com/questions/44164749/how-does-keras-handle-multilabel-classification)
* **Focal Loss**

  **Focal Loss** was introduced by Lin et al., from Facebook, in [this paper](https://arxiv.org/abs/1708.02002). They claim to improve one-stage object detectors using **Focal Loss** to train a detector they name RetinaNet. **Focal loss** is a **Cross-Entropy Loss** that weighs the contribution of each sample to the loss based in the classification error. The idea is that, if a sample is already classified correctly by the CNN, its contribution to the loss decreases. With this strategy, they claim to solve the problem of class imbalance by making the loss implicitly focus in those problematic classes.  
   Moreover, they also weight the contribution of each class to the lose in a more explicit class balancing. They use Sigmoid activations, so **Focal loss** could also be considered a **Binary Cross-Entropy Loss**  
  
  [**https://gombru.github.io/2018/05/23/cross\_entropy\_loss/**](https://gombru.github.io/2018/05/23/cross_entropy_loss/)\*\*\*\*

* \*\*\*\*[**https://www.pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/**](https://www.pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/) **\(also in the 480 todo\)**
* **Is there a relationship between augmented data and new data both creating the same effect?**
  * **Does the addition of 100,000 new images work better than the addition of 100,000 augmentations? Is this a good question?**
* \*\*\*\*[**https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/**](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) **\(really good for explanations of different activation and regularisation techniques\)**
* \*\*\*\*[**https://spinningup.openai.com/en/latest/user/introduction.hml\#introduction**](https://spinningup.openai.com/en/latest/user/introduction.html#introduction) **This whole spinningup stuff covers just about everything!**
* **rlhard** [**https://www.alexirpan.com/2018/02/14/rl-hard.html**](https://www.alexirpan.com/2018/02/14/rl-hard.html) **\(very good!\)**

